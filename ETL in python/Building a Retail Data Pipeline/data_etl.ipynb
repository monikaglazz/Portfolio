{"cells":[{"cell_type":"markdown","id":"ef36f535-4bdc-4e2b-a22a-179372324b26","metadata":{},"source":["\n","Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n","\n","This project will contain creating a data pipeline for the analysis of demand and supply around the holidays and running preliminary analysis of the data. I has two data sources: the `grocery_sales` table in `PostgreSQL` database and `extra_data.parquet` file that contains complementary data. \n","\n","Here is information about all the available columns in the two data files:\n","- `\"index\"` - unique ID of the row\n","- `\"Store_ID\"` - the store number\n","- `\"Date\"` - the week of sales\n","- `\"Weekly_Sales\"` - sales for the given store\n","- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n","- `\"Temperature\"` - Temperature on the day of sale\n","- `\"Fuel_Price\"` - Cost of fuel in the region\n","- `\"CPI\"` â€“ Prevailing consumer price index\n","- `\"Unemployment\"` - The prevailing unemployment rate\n","- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n","- `\"Dept\"` - Department Number in each store\n","- `\"Size\"` - size of the store\n","- `\"Type\"` - type of the store (depends on `Size` column)\n"]},{"cell_type":"code","execution_count":22,"id":"df7b4c2e-25fc-4986-9af9-2718e8e95e6b","metadata":{"executionCancelledAt":null,"executionTime":471,"lastExecutedAt":1696678639079,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required packages\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport logging\nimport os\n\n# create engine\n# engine = create_engine('postgresql://user:password@localhost:5432/database')\n\n# save imported data from table as DataFrame\n# store_df = pd.read_sql('SELECT * FROM grocery_sales', engine)\n\n# Create the extract() function with two parameters: one for the store data and the other one for the extra data\n# Read the extra data from the parquet file and merge the DataFrames using \"index\" column\ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(store_df, \"extra_data.parquet\")\n\n# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n  # Fill NaNs using mean\n  # Set inplace = True to do the replacing on the current DataFrame\n    raw_data.fillna(\n      {\n          'CPI': raw_data['CPI'].mean(),\n          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n          'Unemployment': raw_data['Unemployment'].mean(),\n      }, inplace = True\n    )\n    # Define the type of the \"Date\" column and its format\n    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n\n    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n    \n    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n    return raw_data\n    \n    \n# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)\n\ndef avg_monthly_sales(clean_data):\n    clean_data = clean_data[[\"Month\", \"Weekly_Sales\"]].groupby([\"Month\"]).agg(\n        Avg_Sales=(\"Weekly_Sales\", \"mean\")).reset_index().round(2)\n    return clean_data\n\nagg_data = avg_monthly_sales(clean_data)\n\n# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n    full_data.to_csv(full_data_file_path, index = False)\n    agg_data.to_csv(agg_data_file_path, index = False)\n\n# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \nload(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")\n\ndef validation(file_path):\n    if not os.path.exists(file_path):\n        raise Exception(f\"There is no file at the path {file_path}\")\n\nvalidation(\"clean_data.csv\")\nvalidation(\"agg_data.csv\")\n        "},"outputs":[],"source":["# Import required packages\n","from sqlalchemy import create_engine\n","import pandas as pd\n","import numpy as np\n","import logging\n","import os\n","\n","# Create engine\n","engine = create_engine('postgresql://user:password@localhost:5432/database')\n","\n","# Save imported data from table as DataFrame\n","store_df = pd.read_sql('SELECT * FROM grocery_sales', engine)\n","\n","# Create the extract() function with two parameters: one for the store data and the other one for the extra data\n","# Read the extra data from the parquet file and merge the DataFrames using \"index\" column\n","def extract(store_data, extra_data):\n","    extra_df = pd.read_parquet(extra_data)\n","    merged_df = store_data.merge(extra_df, on = \"index\")\n","    return merged_df\n","\n","# Call the extract() function and store it as the \"merged_df\" variable\n","merged_df = extract(store_df, \"extra_data.parquet\")\n","\n","# Create the transform() function with one parameter: \"raw_data\"\n","def transform(raw_data):\n","  # Fill NaNs using mean\n","  # Set inplace = True to do the replacing on the current DataFrame\n","    raw_data.fillna(\n","      {\n","          'CPI': raw_data['CPI'].mean(),\n","          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n","          'Unemployment': raw_data['Unemployment'].mean(),\n","      }, inplace = True\n","    )\n","    # Define the type of the \"Date\" column and its format\n","    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n","    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n","    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n","\n","    # Filter the entire DataFrame using the \"Weekly_Sales\" column\n","    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n","    \n","    # Drop unnecessary columns\n","    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n","    return raw_data\n","    \n","    \n","# Call the transform() function and pass the merged DataFrame\n","clean_data = transform(merged_df)\n","\n","def avg_monthly_sales(clean_data):\n","    clean_data = clean_data[[\"Month\", \"Weekly_Sales\"]].groupby([\"Month\"]).agg(\n","        Avg_Sales=(\"Weekly_Sales\", \"mean\")).reset_index().round(2)\n","    return clean_data\n","\n","agg_data = avg_monthly_sales(clean_data)\n","\n","# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n","def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n","  \t# Save both DataFrames as csv files\n","    full_data.to_csv(full_data_file_path, index = False)\n","    agg_data.to_csv(agg_data_file_path, index = False)\n","\n","# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n","load(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")\n","\n","# Create validation() to check if files exists\n","def validation(file_path):\n","    if not os.path.exists(file_path):\n","        raise Exception(f\"There is no file at the path {file_path}\")\n","\n","validation(\"clean_data.csv\")\n","validation(\"agg_data.csv\")\n","        "]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
